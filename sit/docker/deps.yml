version: '2.4'
services:
  mysql:
    image: mysql:8
    container_name: mysql
    ports:
      - "3307:3306"
    volumes:
      - ../../third_party/mysql/init/init.sql:/docker-entrypoint-initdb.d/test.sql
      - ../../third_party/mysqld.cnf:/etc/mysql/mysql.conf.d/mysqld.cnf
    restart: always
    environment:
       MYSQL_ROOT_PASSWORD: root
    # networks:
    #   - bridge
    healthcheck:
      test: ["CMD", "mysql" ,"-h", "mysql", "-P", "3306", "-u", "root", "-proot" , "-e", "SELECT 1"]
      interval: 2s
      timeout: 5s
      retries: 30
  etcd:
    image: bitnami/etcd:3.4.13
    container_name: etcd
    ports:
      - "2379:2379"
      - "2380:2380"
    environment:
      - ALLOW_NONE_AUTHENTICATION=yes
      - ETCD_ADVERTISE_CLIENT_URLS=http://etcd:2379
    healthcheck:
      test: ["CMD", "etcdctl", "member", "list", '--endpoints="localhost:2379"']
      interval: 2s
      timeout: 5s
      retries: 30
  redis:
    image: redis:latest
    container_name: redis
    ports:
      - 6379:6379
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 1s
      timeout: 3s
      retries: 30
  zookeeper:
    image: zookeeper:3.6
    container_name: zookeeper
    ports:
      - 2181:2181
    healthcheck:
      test: ["CMD", "nc", "-z", "localhost", "2181"]
      interval: 2s
      timeout: 5s
      retries: 30
    environment:
      # The number of milliseconds of each tick
      - ZOO_TICK_TIME=2000
      # The number of ticks that the initial
      # synchronization phase can take
      # This value is not quite motivated
      - ZOO_INIT_LIMIT=300 
      # The number of ticks that can pass between
      # sending a request and getting an acknowledgement
      - ZOO_SYNC_LIMIT=10
      - ZOO_MAX_CLIENT_CNXNS=2000
      - ZOO_AUTOPURGE_SNAPRETAINCOUNT=10
      - ZOO_AUTOPURGE_PURGEINTERVAL=1
      # To avoid seeks ZooKeeper allocates space in the transaction log file in
      # blocks of preAllocSize kilobytes. The default block size is 64M. One reason
      # for changing the size of the blocks is to reduce the block size if snapshots
      # are taken more often. (Also, see snapCount).
      - ZOO_PRE_ALLOC_SIZE=131072
      # Clients can submit requests faster than ZooKeeper can process them,
      # especially if there are a lot of clients. To prevent ZooKeeper from running
      # out of memory due to queued requests, ZooKeeper will throttle clients so that
      # there is no more than globalOutstandingLimit outstanding requests in the
      # system. The default limit is 1,000.ZooKeeper logs transactions to a
      # transaction log. After snapCount transactions are written to a log file a
      # snapshot is started and a new transaction log file is started. The default
      # snapCount is 10,000.
      - ZOO_SNAP_COUNT=3000000
      # - ZOO_STANDALONE_ENABLED
      # - ZOO_ADMINSERVER_ENABLED
      # - ZOO_4LW_COMMANDS_WHITELIST
      # - ZOO_ADMINSERVER_ENABLED

#  ch-server-1:
#      image: yandex/clickhouse-server
#      container_name: ch-server-1
#      volumes:
#        - ./clickhouse/config.xml:/etc/clickhouse-server/config.d/local.xml
#        - ./clickhouse/macros1.xml:/etc/clickhouse-server/config.d/macros.xml
#      ports:
#        - 8123:8123
#        - 9000:9000
#      environment:
#        TZ: "Asia/Shanghai"
#      depends_on:
#        mysql:
#          condition: service_healthy
#        redis:
#          condition: service_healthy
#        etcd:
#          condition: service_healthy
#        zookeeper:
#          condition: service_healthy
#  ch-server-2:
#    image: yandex/clickhouse-server
#    container_name: ch-server-2
#    volumes:
#      - ./clickhouse/config.xml:/etc/clickhouse-server/config.d/local.xml
#      - ./clickhouse/macros2.xml:/etc/clickhouse-server/config.d/macros.xml
#    ports:
#      - 8124:8123
#      - 9001:9000
#    environment:
#      TZ: "Asia/Shanghai"
#    depends_on:
#      mysql:
#        condition: service_healthy
#      redis:
#        condition: service_healthy
#      etcd:
#        condition: service_healthy
#      zookeeper:
#        condition: service_healthy
#  ch-server-3:
#    image: yandex/clickhouse-server
#    container_name: ch-server-3
#    volumes:
#      - ./clickhouse/config.xml:/etc/clickhouse-server/config.d/local.xml
#      - ./clickhouse/macros3.xml:/etc/clickhouse-server/config.d/macros.xml
#    ports:
#      - 8125:8123
#      - 9002:9000
#    environment:
#      TZ: "Asia/Shanghai"
#    depends_on:
#      mysql:
#        condition: service_healthy
#      redis:
#        condition: service_healthy
#      etcd:
#        condition: service_healthy
#      zookeeper:
#        condition: service_healthy
#  ch-server-4:
#    image: yandex/clickhouse-server
#    container_name: ch-server-4
#    volumes:
#      - ./clickhouse/config.xml:/etc/clickhouse-server/config.d/local.xml
#      - ./clickhouse/macros4.xml:/etc/clickhouse-server/config.d/macros.xml
#    ports:
#      - 8126:8123
#      - 9003:9000
#    environment:
#      TZ: "Asia/Shanghai"
#    depends_on:
#      mysql:
#        condition: service_healthy
#      redis:
#        condition: service_healthy
#      etcd:
#        condition: service_healthy
#      zookeeper:
#        condition: service_healthy
#  mongo1:
#    image: mongo:4.4
#    container_name: mongo1
#    command: ["--replSet", "my-replica-set", "--bind_ip_all", "--port", "30001"]
#    # volumes:
#      # - ./data/mongo-1:/data/db
#    ports:
#      - "30001:30001"
#    healthcheck:
##      test: ["CMD", "ps -ef | grep 30001"]
#      test: test $$(echo "rs.initiate({_id:'my-replica-set',members:[{_id:0,host:\"mongo1:30001\"},{_id:1,host:\"mongo2:30002\"},{_id:2,host:\"mongo3:30003\"}]}).ok || rs.status().ok" | mongo --port 30001 --quiet) -eq 1
#      interval: 40s
#      start_period: 60s

#  mongo2:
#    image: mongo:4.4
#    container_name: mongo2
#    command: ["--replSet", "my-replica-set", "--bind_ip_all", "--port", "30002"]
#    # volumes:
#      # - ./data/mongo-2:/data/db
#    ports:
#      - "30002:30002"

#  mongo3:
#    image: mongo:4.4
#    container_name: mongo3
#    command: ["--replSet", "my-replica-set", "--bind_ip_all", "--port", "30003"]
#    # volumes:
#      # - ./data/mongo-3:/data/db
#    ports:
#      - "30003:30003"
  kafka:
    image: wurstmeister/kafka
    restart: always
    ports:
      - 9092:9092
    environment:
      - ZOOKEEPER_IP=zookeeper
      - KAFKA_BROKER_ID=1
      - KAFKA_LISTENERS=PLAINTEXT://:9092
      - KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092
      - KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181
      - ALLOW_PLAINTEXT_LISTENER=yes
    healthcheck:
      test: [ "CMD", "nc", "-z", "localhost", "9092" ]
      interval: 2s
      timeout: 5s
      retries: 30
  kafka-connector:
    image: liliankasem/kafka-connect:1.1.0
    container_name: kafka-connector
    hostname: connect
#    container_name: connect
    restart: always
    ports:
      - "8083:8083"
    environment:
      CONNECT_BOOTSTRAP_SERVERS: kafka:9092
      CONNECT_REST_ADVERTISED_HOST_NAME: connect
      CONNECT_REST_PORT: 8083
      CONNECT_GROUP_ID: compose-connect-group
      CONNECT_CONFIG_STORAGE_TOPIC: docker-connect-configs
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_OFFSET_FLUSH_INTERVAL_MS: 10000
      CONNECT_OFFSET_STORAGE_TOPIC: docker-connect-offsets
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_STATUS_STORAGE_TOPIC: docker-connect-status
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_KEY_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_INTERNAL_KEY_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_INTERNAL_VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_PLUGIN_PATH: /usr/share/java
    healthcheck:
      test: [ "CMD", "nc", "-z", "kafka-connector", "8083" ]
      interval: 2s
      timeout: 5s
      retries: 30
    volumes:
      - ../../third_party/mongo-kafka-1.2.0-all.jar:/usr/share/java/mongo-kafka-1.2.0-all.jar
    depends_on:
      kafka:
        condition: service_healthy
  canal-server:
    image: canal/canal-server
    container_name: canal-server
    restart: always
    environment:
      canal.admin.user: admin
      canal.admin.port: 11110
      canal.admin.passwd: 4ACFE3202A5FF5CF467898FC58AAB1D615029441
      canal.admin.manager: canal-admin:8089
  canal-admin:
    image: canal/canal-admin
    container_name: canal-admin
    ports:
      - 8089:8089
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://canal-admin:8089" ]
      interval: 30s
      timeout: 15s
      retries: 30
    restart: always    
    depends_on: 
      - canal-server
    volumes:
      - ../../third_party/canal_admin_conf/canal-template.properties:/home/admin/canal-admin/conf/canal-template.properties
